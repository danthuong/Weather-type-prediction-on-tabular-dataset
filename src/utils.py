import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pathlib
import json
from typing import List, Optional
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score, accuracy_score, f1_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline as Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder, label_binarize

def resolve_root():
    here = pathlib.Path.cwd().resolve()
    for c in [here, *list(here.parents)[:3]]:
        if (c / 'data' / 'raw' / 'weather_classification_data.csv').exists():
            return c
    return here

# def make_preprocessor(
#     numeric_features: List[str],
#     categorical_features: List[str],
#     log1p_features: Optional[List[str]] = None,
# ) -> ColumnTransformer:
#     log1p_features = list(log1p_features or [])
#     num_no_log = [f for f in numeric_features if f not in log1p_features]

#     transformers = []

#     if log1p_features:
#         transformers.append(
#             (
#                 'num_log1p',
#                 SKPipeline(
#                     steps=[
#                         ('log1p', FunctionTransformer(np.log1p, feature_names_out='one-to-one')),
#                         ('scaler', StandardScaler()),
#                     ]
#                 ),
#                 log1p_features,
#             )
#         )

#     if num_no_log:
#         transformers.append(('num', StandardScaler(), num_no_log))

#     if categorical_features:
#         transformers.append(
#             ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
#         )

#     preprocessor = ColumnTransformer(transformers)
#     return preprocessor

def make_preprocessor(cat_features, num_features, log1p_features=None):
    """T·∫°o preprocessor g·ªìm x·ª≠ l√Ω numeric, categorical, v√† log1p (n·∫øu c√≥)."""

    log1p_features = log1p_features or []
    remaining_num = [f for f in num_features if f not in log1p_features]

    # Numeric pipeline
    num_pipeline = Pipeline([
        ("scaler", StandardScaler())
    ])

    # Log1p pipeline (d√†nh cho c√°c c·ªôt skew cao)
    log1p_pipeline = Pipeline([
        ("log1p", FunctionTransformer(np.log1p, validate=False)),
        ("scaler", StandardScaler())
    ])

    # Categorical pipeline
    cat_pipeline = Pipeline([
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", num_pipeline, remaining_num),
            ("log1p", log1p_pipeline, log1p_features),
            ("cat", cat_pipeline, cat_features)
        ],
        remainder="drop"
    )

    return preprocessor

def eval_on_val(model_key,
                base_estimator,
                grid,
                preprocessor,
                X_train,
                y_train,
                X_val,
                y_val,
                label_encoder: Optional[LabelEncoder] = None):
    """
    ƒê√°nh gi√° c√°c c·∫•u h√¨nh hyperparameter tr√™n t·∫≠p validation.
    Gi·ªØ nguy√™n logic ban ƒë·∫ßu: cho m·ªói cfg -> t·∫°o Pipeline(prep, clf),
    set c√°c tham s·ªë cho clf, fit tr√™n X_train/y_train_enc, predict X_val,
    t√≠nh macro_f1 v√† accuracy d·ª±a tr√™n y_val_enc.

    N·∫øu y_train / y_val l√† nh√£n kh√¥ng-encoded (chu·ªói ho·∫∑c s·ªë),
    h√†m s·∫Ω t·ª± t·∫°o LabelEncoder (ho·∫∑c d√πng label_encoder n·∫øu truy·ªÅn v√†o).

    Parameters
    ----------
    model_key : str
        T√™n m√¥ h√¨nh (v√≠ d·ª• "svm", "xgb", "logreg")
    base_estimator : estimator instance
        M·ªôt instance c·ªßa estimator (d√πng ƒë·ªÉ l·∫•y class v√† default params)
    grid : list of dict
        Danh s√°ch c·∫•u h√¨nh: [{"config_id": "...", "params": {...}}, ...]
    preprocessor : transformer
        Preprocessing pipeline (ColumnTransformer / Pipeline)
    X_train, y_train, X_val, y_val : array-like / DataFrame / Series
        D·ªØ li·ªáu th√¥ (y c√≥ th·ªÉ l√† chu·ªói)
    label_encoder : sklearn.preprocessing.LabelEncoder or None
        N·∫øu truy·ªÅn v√†o, s·∫Ω d√πng n√≥; n·∫øu None, h√†m s·∫Ω t·∫°o + fit tr√™n y_train.

    Returns
    -------
    pd.DataFrame
        C√°c h√†ng ch·ª©a model, config_id, params_json, macro_f1_val, accuracy_val
    """
    rows = []

    # --- label encoding (n·∫øu c·∫ßn) ---
    if label_encoder is None:
        le = LabelEncoder()
        # fit on y_train (assumes y_train is array-like)
        y_train_arr = np.array(y_train)
        le.fit(y_train_arr)
    else:
        le = label_encoder

    # transform labels
    y_train_enc = le.transform(np.array(y_train))
    y_val_enc = le.transform(np.array(y_val))

    # iterate grid (same logic as c·ªßa b·∫°n)
    for cfg in grid:
        params = cfg["params"].copy()
        cfg_id = cfg["config_id"]

        # instantiate a fresh estimator of same class with same init params
        est_cls = base_estimator.__class__
        est = est_cls(**base_estimator.get_params())

        # pipeline: preprocessor + fresh estimator
        pipe = Pipeline(steps=[("prep", preprocessor), ("clf", est)])

        # set hyperparameters on the classifier step
        if params:
            pipe.set_params(**{f"clf__{k}": v for k, v in params.items()})

        # fit on training (encoded)
        pipe.fit(X_train, y_train_enc)

        # predict on validation
        y_pred = pipe.predict(X_val)

        # compute metrics on encoded labels
        macro_f1 = f1_score(y_val_enc, y_pred, average="macro")
        acc = accuracy_score(y_val_enc, y_pred)

        rows.append({
            "model": model_key,
            "config_id": cfg_id,
            "params_json": json.dumps(params),
            "macro_f1_val": macro_f1,
            "accuracy_val": acc,
        })

    return pd.DataFrame(rows)

# def eval_on_val(model_key, base_estimator, grid):
#     rows = []
#     for cfg in grid:
#         params = cfg["params"].copy(); cfg_id = cfg["config_id"]
#         pipe = Pipeline(steps=[("prep", preprocessor), ("clf", base_estimator.__class__(**base_estimator.get_params()))])
#         pipe.set_params(**{f"clf__{k}": v for k,v in params.items()})
#         pipe.fit(X_train, y_train_enc)
#         y_pred = pipe.predict(X_val)
#         rows.append({
#             "model": model_key, "config_id": cfg_id, "params_json": json.dumps(params),
#             "macro_f1_val": f1_score(y_val_enc, y_pred, average="macro"),
#             "accuracy_val": accuracy_score(y_val_enc, y_pred),
#         })
#     return pd.DataFrame(rows)

def softmax(logits):
    # logits: (n_samples, n_classes)
    e = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    return e / np.sum(e, axis=1, keepdims=True)

def evaluate_and_plot(model, X_test, y_test, label_encoder=None, feature_names=None):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print("Accuracy:", acc)
    if label_encoder is not None:
        target_names = label_encoder.classes_
    else:
        # numeric class names
        target_names = [str(i) for i in range(model.n_classes)]

    print("\nClassification report:")
    print(classification_report(y_test, y_pred, target_names=target_names))

    # confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title("Confusion matrix")
    plt.colorbar()
    tick_marks = np.arange(len(target_names))
    plt.xticks(tick_marks, target_names, rotation=45)
    plt.yticks(tick_marks, target_names)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > cm.max()/2. else "black")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

    # feature importance bar
    fi = model.get_feature_importance(feature_names=feature_names, top_n=30)
    if len(fi) > 0:
        names, vals = zip(*fi)
        plt.figure(figsize=(8, max(3, len(vals)*0.3)))
        y_pos = np.arange(len(vals))
        plt.barh(y_pos, vals[::-1])
        plt.yticks(y_pos, names[::-1])
        plt.title("Feature importance (gain, aggregated)")
        plt.xlabel("Total Gain")
        plt.tight_layout()
        plt.show()

# --- H√†m v·∫Ω v√† l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n ---
def plot_and_save_confusion(cm, classes, title, path):
    """
    V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n v√† l∆∞u ·∫£nh.

    Parameters
    ----------
    cm : np.ndarray
        Ma tr·∫≠n nh·∫ßm l·∫´n
    classes : list of str
        T√™n l·ªõp
    title : str
        Ti√™u ƒë·ªÅ bi·ªÉu ƒë·ªì
    path : str / pathlib.Path
        ƒê∆∞·ªùng d·∫´n l∆∞u file
    """
    plt.figure(figsize=(5.2,4.2))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=classes, yticklabels=classes)
    plt.xlabel("D·ª± ƒëo√°n")
    plt.ylabel("Th·ª±c t·∫ø")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

def roc_pr_ovr(y_true_enc, y_proba, classes, prefix, out_dir):
    """
    V·∫Ω ROC & PR (one-vs-rest) cho nhi·ªÅu l·ªõp v√† l∆∞u ·∫£nh.

    Parameters
    ----------
    y_true_enc : np.ndarray
        Nh√£n true (encoded)
    y_proba : np.ndarray
        D·ª± ƒëo√°n probability t·ª´ classifier
    classes : list of str
        Danh s√°ch t√™n l·ªõp
    prefix : str
        Ti·ªÅn t·ªë t√™n m√¥ h√¨nh
    out_dir : str / pathlib.Path
        Th∆∞ m·ª•c l∆∞u h√¨nh
    """
    # ROC
    plt.figure(figsize=(5.2,4.2))
    for i, cls in enumerate(classes):
        fpr, tpr, _ = roc_curve(y_true_enc==i, y_proba[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{cls} (AUC={roc_auc:.3f})")
    plt.plot([0,1],[0,1],'k--')
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.title(f"ROC OvR ‚Äî {prefix}")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_dir / f"{prefix}_test_roc.png", dpi=150)
    plt.close()

    # PR
    plt.figure(figsize=(5.2,4.2))
    for i, cls in enumerate(classes):
        p, r, _ = precision_recall_curve(y_true_enc==i, y_proba[:, i])
        ap = average_precision_score(y_true_enc==i, y_proba[:, i])
        plt.plot(r, p, label=f"{cls} (AP={ap:.3f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(f"PR OvR ‚Äî {prefix}")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_dir / f"{prefix}_test_pr.png", dpi=150)
    plt.close()

def plot_composite_evaluation(model_key, model_pipe, X_test, y_test_enc, classes, out_dir, model_map=None):
    """
    V·∫Ω h√¨nh k·∫øt h·ª£p: B√™n tr√°i l√† Confusion Matrix, B√™n ph·∫£i l√† ROC Curve (One-vs-Rest)
    
    Args:
        model_map (dict, optional): Dict √°nh x·∫° t·ª´ key sang t√™n hi·ªÉn th·ªã ƒë·∫πp h∆°n. 
                                    V√≠ d·ª•: {'xgb': 'XGBoost'}.
    """
    # 1. D·ª± ƒëo√°n
    y_pred = model_pipe.predict(X_test)
    y_proba = model_pipe.predict_proba(X_test) if hasattr(model_pipe.named_steps["clf"], "predict_proba") else None
    
    # 2. X√°c ƒë·ªãnh t√™n hi·ªÉn th·ªã
    if model_map:
        model_name = model_map.get(model_key, model_key.upper())
    else:
        model_name = model_key.upper()
    
    # 3. Chu·∫©n b·ªã figure
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # --- SUBPLOT 1: CONFUSION MATRIX ---
    cm = confusion_matrix(y_test_enc, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=classes, yticklabels=classes, ax=axes[0],
                annot_kws={"size": 14})
    axes[0].set_title(f"{model_name} Confusion Matrix", fontsize=14)
    axes[0].set_xlabel("Predicted", fontsize=12)
    axes[0].set_ylabel("True", fontsize=12)
    
    # --- SUBPLOT 2: ROC CURVE ---
    if y_proba is not None:
        # Binarize labels cho One-vs-Rest
        y_test_bin = label_binarize(y_test_enc, classes=range(len(classes)))
        
        for i, cls in enumerate(classes):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
            roc_auc = auc(fpr, tpr)
            axes[1].plot(fpr, tpr, lw=2, label=f'{cls} (AUC = {roc_auc:.4f})')
            
        axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
        axes[1].set_xlim([0.0, 1.0])
        axes[1].set_ylim([0.0, 1.05])
        axes[1].set_xlabel('False Positive Rate', fontsize=12)
        axes[1].set_ylabel('True Positive Rate', fontsize=12)
        axes[1].set_title(f"{model_name} ROC Curve (One-vs-Rest)", fontsize=14)
        axes[1].legend(loc="lower right")
        axes[1].grid(True, alpha=0.3)
    else:
        axes[1].text(0.5, 0.5, "Model does not support probability", 
                     ha='center', va='center', transform=axes[1].transAxes)

    plt.tight_layout()
    
    # L∆∞u h√¨nh ·∫£nh
    save_path = out_dir / f"{model_key}_test_combined.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"üì∏ Saved combined plot for {model_key}: {save_path}")
    plt.show()